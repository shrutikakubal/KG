"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IMLORzG7RQDdOIQa92VDJE5UcPeK-VkQ
"""
#NLP library - Sentence Detection, Frequent Word Removal, Type-of-word detection (noun, action etc), lemmatization (reduce to root), similarity
import spacy
import numpy as np
import pandas as pd #CSV file I/O (e.g. pd.read_csv)
import os
file_name = "wiki_sentences_v2.csv"
with open(file_name, 'r') as file:
    content = file.read()
#Load nlp model - sm=trained with less data
nlp = spacy.load('en_core_web_sm')
#create nlp object - tokenize the text
doc = nlp("The meeting is on friday")
#create dependency relation (syntactic) between word and parent
for tok in doc:
    print(tok.text, "...", tok.dep_)

"""# New Section"""

#Named Entity Recognition
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

# Commented out IPython magic to ensure Python compatibility.
import re
import pandas as pd
import bs4
import requests
import spacy
from spacy import displacy
#nlp = spacy.load('en_core_web_sm')
from spacy.matcher import Matcher
from spacy.tokens import Span
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import tqdm
pd.set_option('display.max_colwidth', 200)
# %matplotlib inline

sentences=pd.read_csv(file_name)
print(sentences)

#entity pair extraction
def get_entities(sent):
    ## chunk 1
    ent1 = ""
    ent2 = ""

    prv_tok_dep = ""  # dependency tag of previous token in the sentence
    prv_tok_text = ""  # previous token in the sentence

    prefix = ""
    modifier = ""

    for tok in nlp(sent):
        ## chunk 2
        # if token is a punctuation mark then move on to the next token
        if tok.dep_ != "punct":
            # check: token is a compound word or not
            if tok.dep_ == "compound":
                prefix = tok.text
                # if the previous word was also a 'compound' then add the current word to it
                if prv_tok_dep == "compound":
                    prefix = prv_tok_text + " " + tok.text

            # check: token is a modifier (adjective, quantifier, "very" before "very quickly") or not
            if tok.dep_.endswith("mod") == True:
                modifier = tok.text
                # if the previous word was also a 'compound' then add the current word to it (e.g. "goodbye, highrise")
                if prv_tok_dep == "compound":
                    modifier = prv_tok_text + " " + tok.text

            ## chunk 3 - find subject (entity one)
            if tok.dep_.find("subj") == True:
                ent1 = modifier + " " + prefix + " " + tok.text
                prefix = ""
                modifier = ""
                prv_tok_dep = ""
                prv_tok_text = ""

                ## chunk 4 - find object (entity two)
            if tok.dep_.find("obj") == True:
                ent2 = modifier + " " + prefix + " " + tok.text

            ## chunk 5
            # update variables
            prv_tok_dep = tok.dep_
            prv_tok_text = tok.text
    #############################################################

    return [ent1.strip(), ent2.strip()]

get_entities("I have so much work")

entity_pairs = []

#tqdm -> show progress bar
for i in tqdm(sentences["sentence"]):
    entity_pairs.append(get_entities(i))

def  get_relation(sent):

    doc = nlp(sent)

    # Matcher class object
    matcher = Matcher(nlp.vocab)

    #find root, prepositional modifier, agent (do-er), adjective
    pattern = [{'DEP':'ROOT'},
            {'DEP':'prep','OP':"?"},
            {'DEP':'agent','OP':"?"},
            {'POS':'ADJ','OP':"?"}]

    matcher.add("matching_1",[pattern])

    matches = matcher(doc)
    k = len(matches) - 1

    #last matching substring
    span = doc[matches[k][1]:matches[k][2]]

    return(span.text)

get_relation("Shruti went for a run")

relations = [get_relation(i) for i in tqdm(sentences['sentence'])]

#Series (key-value pairs) containing the counts of the 50 most frequent relations
#pd.Series(relations).value_counts()[:50]

#entity_pairs[0:10]
#relations[0:10]
# extract subject
source = [i[0] for i in entity_pairs]
# extract object
target = [i[1] for i in entity_pairs]

kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})
kg_df[0:10]

#create graph
G=nx.from_pandas_edgelist(kg_df, "source", "target",
                          edge_attr=True, create_using=nx.MultiDiGraph())
plt.figure(figsize=(12,12))

pos = nx.spring_layout(G)
nx.draw(G, with_labels=True, node_color='blue', edge_cmap=plt.cm.Blues, pos = pos)
plt.show()

#Filter on some relation
G=nx.from_pandas_edgelist(kg_df[kg_df['edge']=="produced"], "source", "target",
                          edge_attr=True, create_using=nx.MultiDiGraph())

plt.figure(figsize=(10,10))
pos = nx.spring_layout(G, k = 0.5) # bigger k = more distance between nodes
nx.draw(G, with_labels=True, node_color='blue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos, font_weight='bold')
plt.show()
