{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQxRGi6Q1O0uv/KbOQY57c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrutikakubal/KG/blob/main/kg2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AxYYncduXTM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea491114-6bd8-49a5-e3c4-64bc7b6b832d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.7.3\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.7.3) (3.7.4)\n",
            "Collecting spacy-curated-transformers<0.3.0,>=0.2.0 (from en-core-web-trf==3.7.3)\n",
            "  Downloading spacy_curated_transformers-0.2.2-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.3/236.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.25.2)\n",
            "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3)\n",
            "  Downloading curated_tokenizers-0.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.1.0+cu121)\n",
            "Requirement already satisfied: regex>=2022 in /usr/local/lib/python3.10/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2023.12.25)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
            "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
            "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.7.3 spacy-curated-transformers-0.2.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "#!pip install spacy-transformers\n",
        "import spacy\n",
        "#BERT\n",
        "!python -m spacy download en_core_web_trf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# Extract named entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "# Print results\n",
        "if entities:\n",
        "   print(\"NamedEntities:\")\n",
        "   for entity, label in entities:\n",
        "       print(f\"{entity} - {label}\")\n",
        "else:\n",
        "  print(\"No named entities found in the sentence.\")\n",
        "for tok in doc:\n",
        "    print(tok.text, \"...\", tok.dep_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHmkrH6bnTHd",
        "outputId": "01a0a894-4ca0-4b50-afcd-c344f1f11e85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NamedEntities:\n",
            "Apple - ORG\n",
            "Apple ... compound\n",
            "shares ... nsubj\n",
            "rose ... ROOT\n",
            "on ... prep\n",
            "the ... det\n",
            "news ... pobj\n",
            ". ... punct\n",
            "Apple ... compound\n",
            "pie ... nsubj\n",
            "is ... ROOT\n",
            "delicious ... acomp\n",
            ". ... punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#entity pair extraction\n",
        "def get_entities(sent):\n",
        "    ## chunk 1\n",
        "    ent1 = \"\"\n",
        "    ent2 = \"\"\n",
        "\n",
        "    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n",
        "    prv_tok_text = \"\"  # previous token in the sentence\n",
        "    prev_subject = \"\" # prev subject\n",
        "    prev_object = \"\" # prev object\n",
        "\n",
        "    prefix = \"\"\n",
        "    modifier = \"\"\n",
        "    label = \"\"\n",
        "\n",
        "    for tok in nlp(sent):\n",
        "        ## chunk 2\n",
        "        # if token is a punctuation mark then move on to the next token\n",
        "        if tok.dep_ != \"punct\":\n",
        "            # check: token is a compound word or not\n",
        "            if tok.dep_ == \"compound\":\n",
        "                prefix = tok.text\n",
        "                # if the previous word was also a 'compound' then add the current word to it\n",
        "                if prv_tok_dep == \"compound\":\n",
        "                    prefix = prv_tok_text + \" \" + tok.text\n",
        "\n",
        "            # check: token is a modifier (adjective, quantifier, \"very\" before \"very quickly\") or not\n",
        "            if tok.dep_.endswith(\"mod\") == True:\n",
        "                modifier = tok.text\n",
        "                # if the previous word was also a 'compound' then add the current word to it (e.g. \"goodbye, highrise\")\n",
        "                if prv_tok_dep == \"compound\":\n",
        "                    modifier = prv_tok_text + \" \" + tok.text\n",
        "\n",
        "            ## chunk 3 - find subject (entity one)\n",
        "            if tok.dep_.find(\"subj\") == True:\n",
        "              if tok.pos_==\"PRON\" and prev_subject!=\"\":\n",
        "                ent1 = prev_subject\n",
        "              else:\n",
        "                ent1 = modifier + \" \" + prefix + \" \" + tok.text\n",
        "              prefix = \"\"\n",
        "              modifier = \"\"\n",
        "              prv_tok_dep = \"\"\n",
        "              prv_tok_text = \"\"\n",
        "              prev_subject = ent1\n",
        "\n",
        "                ## chunk 4 - find object (entity two)\n",
        "            if tok.dep_.find(\"obj\") == True:\n",
        "              if tok.pos_==\"PRON\" and prev_object!=\"\":\n",
        "                ent2 = prev_object\n",
        "              else:\n",
        "                ent2 = modifier + \" \" + prefix + \" \" + tok.text\n",
        "                prev_object = ent2\n",
        "\n",
        "              ## chunk 6 - add label\n",
        "            if tok.dep_ != \"ROOT\" and prv_tok_dep == \"dobj\" and tok.dep_== \"acl\":\n",
        "              for child in tok.children:\n",
        "              # Check if the dependent is an adjective clause modifier\n",
        "                if child.dep_ == \"oprd\":\n",
        "                  subtree_text = \"\"\n",
        "                  for sub_token in child.subtree:\n",
        "                    if not sub_token.is_punct:\n",
        "                      subtree_text += sub_token.text + \" \"\n",
        "            # Print the dependent and its subtree\n",
        "                    label = subtree_text.strip()\n",
        "                if ent2!=\"\":\n",
        "                  ent2 = ent2 + \" \" + tok.text + \" \" + label\n",
        "                  prev_object = ent2\n",
        "\n",
        "            ## chunk 5\n",
        "            # update variables\n",
        "            prv_tok_dep = tok.dep_\n",
        "            prv_tok_text = tok.text\n",
        "\n",
        "    #############################################################\n",
        "    return [ent1.strip(), ent2.strip()]\n",
        "\n",
        "\n",
        "def  get_relation(sent):\n",
        "\n",
        "    doc = nlp(sent)\n",
        "\n",
        "    # Matcher class object\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    #find root, prepositional modifier, agent (do-er), adjective\n",
        "    pattern = [{'DEP':'ROOT'},\n",
        "            {'DEP':'prep','OP':\"?\"},\n",
        "            {'DEP':'agent','OP':\"?\"},\n",
        "            {'POS':'ADJ','OP':\"?\"}]\n",
        "\n",
        "    matcher.add(\"matching_1\",[pattern])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "    k = len(matches) - 1\n",
        "\n",
        "    #last matching substring\n",
        "    span = doc[matches[k][1]:matches[k][2]]\n",
        "\n",
        "    return(span.text)"
      ],
      "metadata": {
        "id": "7j2o7OZunygs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentence_by_nsubj(sentence):\n",
        "    # Parse the sentence with spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Initialize variables to store sentence parts\n",
        "    sentence_parts = []\n",
        "    current_part = []\n",
        "\n",
        "    # Iterate through the tokens\n",
        "    for token in doc:\n",
        "        # If the token is a subject, start a new part\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            if current_part:\n",
        "                sentence_parts.append(\" \".join(current_part))\n",
        "                current_part = []\n",
        "        # Add token to the current part\n",
        "        current_part.append(token.text)\n",
        "\n",
        "    # Add the last part to the list\n",
        "    if current_part:\n",
        "        sentence_parts.append(\" \".join(current_part))\n",
        "\n",
        "    return sentence_parts\n",
        "\n",
        "def split_sentence(sentence):\n",
        "    # Parse the sentence with spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Initialize variables to store sentence parts\n",
        "    sentence_parts = []\n",
        "    subject_indices = []\n",
        "# Find the subject and verb indices\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            subject_indices.append(token.i)\n",
        "    # If both subject and verb are found, split the sentence\n",
        "    if(len(subject_indices)>1):\n",
        "      for j in range(len(subject_indices)-1):\n",
        "        # Split the sentence into two parts\n",
        "        if doc[subject_indices[j+1]].pos_==\"PRON\":\n",
        "          text = \" \".join([token.text for token in doc[subject_indices[j]:subject_indices[j+1]]])\n",
        "        else:\n",
        "          text = \" \".join([token.text for token in doc[subject_indices[j]:subject_indices[j+1]+1]])\n",
        "        sentence_parts.append(text)\n",
        "\n",
        "      text = \" \".join([token.text for token in doc[subject_indices[j+1]:]])\n",
        "      sentence_parts.append(text)\n",
        "      return sentence_parts\n",
        "    else:\n",
        "      return sentence"
      ],
      "metadata": {
        "id": "GMNSKu1qmWSQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(get_entities(\"what did lions gate entertainment produce? They produced bambi.\"))\n",
        "#get_relation(\"the film is narrated by chiranjeevi.\")\n",
        "\n",
        "#split_sentence_by_nsubj(\"what did lions gate entertainment produce? They produced bambi.\")\n",
        "doc=nlp(\"Alice finds a fan that enables her to shrink enough to get into the Garden\")\n",
        "for tok in doc:\n",
        "    print(tok.text, \"...\", tok.dep_, \"...\", tok.pos_,)\n",
        "print(split_sentence(\"Alice finds a fan that enables her to shrink enough to get into the Garden\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEFZ5BFVnz0S",
        "outputId": "89a9dca2-9558-45f8-8d76-a2dae1effda9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice ... nsubj ... PROPN\n",
            "finds ... ROOT ... VERB\n",
            "a ... det ... DET\n",
            "fan ... dobj ... NOUN\n",
            "that ... nsubj ... PRON\n",
            "enables ... relcl ... VERB\n",
            "her ... dobj ... PRON\n",
            "to ... aux ... PART\n",
            "shrink ... xcomp ... VERB\n",
            "enough ... advmod ... ADV\n",
            "to ... aux ... PART\n",
            "get ... xcomp ... VERB\n",
            "into ... prep ... ADP\n",
            "the ... det ... DET\n",
            "Garden ... pobj ... PROPN\n",
            "['Alice finds a fan', 'that enables her to shrink enough to get into the Garden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the named entity recognizer (NER) component\n",
        "ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "# Get all the entity types recognized by the model\n",
        "entity_types = ner.labels\n",
        "\n",
        "# Print the entity types\n",
        "print(\"Entity types:\", entity_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voav4It6o-sB",
        "outputId": "f8383d63-91b7-46bf-9226-a597cbcb25a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity types: ('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "paragraph = 'Alice follows a large white rabbit down a \"Rabbit-hole\". Alice finds a tiny door. When Alice finds a bottle labeled \"Drink me\", she does. Alice shrinks, but not enough to pass through the door. Alice then eats something labeled \"Eat me\" and grows larger. Alice finds a fan. that enables her to shrink enough to get into the \"Garden\". Alice tries to get a \"Dog\" to play with her. She enters the \"White Rabbits tiny House,\" but suddenly resumes her normal size. In order to get out, she has to use the \"magic fan.\"'\n",
        "# Split paragraph into sentences by punctuation\n",
        "sentences = paragraph.split('. ')\n",
        "\n",
        "# Create DataFrame\n",
        "sents = pd.DataFrame({'Sentence': sentences})\n",
        "\n",
        "# Print DataFrame\n",
        "print(sents)\n",
        "doc=nlp(\"Alice tries to get a 'Dog' to play with her\")\n",
        "for tok in doc:\n",
        "    print(tok.text, \"...\", tok.dep_, \"...\", tok.pos_,)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBKIvp1XylF9",
        "outputId": "94de8117-0d1e-4b2e-aa7b-470cf40a3ac0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Sentence\n",
            "0  Alice follows a large white rabbit down a \"Rab...\n",
            "1                            Alice finds a tiny door\n",
            "2  When Alice finds a bottle labeled \"Drink me\", ...\n",
            "3  Alice shrinks, but not enough to pass through ...\n",
            "4  Alice then eats something labeled \"Eat me\" and...\n",
            "5                                  Alice finds a fan\n",
            "6  that enables her to shrink enough to get into ...\n",
            "7        Alice tries to get a \"Dog\" to play with her\n",
            "8  She enters the \"White Rabbits tiny House,\" but...\n",
            "9  In order to get out, she has to use the \"magic...\n",
            "Alice ... nsubj ... PROPN\n",
            "tries ... ROOT ... VERB\n",
            "to ... aux ... PART\n",
            "get ... xcomp ... VERB\n",
            "a ... det ... DET\n",
            "' ... punct ... PUNCT\n",
            "Dog ... nsubj ... NOUN\n",
            "' ... punct ... PUNCT\n",
            "to ... aux ... PART\n",
            "play ... ccomp ... VERB\n",
            "with ... prep ... ADP\n",
            "her ... pobj ... PRON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_pairs = []\n",
        "\n",
        "for i in sents[\"Sentence\"]:\n",
        "    entity_pairs.append(get_entities(i))\n",
        "relations = [get_relation(i) for i in sents[\"Sentence\"]]\n"
      ],
      "metadata": {
        "id": "SNE6jj-jzSXB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract subject\n",
        "source = [i[0] for i in entity_pairs]\n",
        "# extract object\n",
        "target = [i[1] for i in entity_pairs]\n",
        "\n",
        "kg_df = pd.DataFrame({'source':source, 'edge':relations, 'target':target})\n",
        "print(kg_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1y6JGkuzygf",
        "outputId": "1a456fed-0c93-473a-ad32-aa3890f2247b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        source     edge                          target\n",
            "0        Alice  follows               white Rabbit hole\n",
            "1        Alice    finds                      tiny  door\n",
            "2  When  Alice     does         bottle labeled Drink me\n",
            "3        Alice  shrinks                    enough  door\n",
            "4        Alice     eats  then  something labeled Eat me\n",
            "5        Alice    finds                             fan\n",
            "6         that  enables                  enough  Garden\n",
            "7          Dog    tries                             her\n",
            "8          She   enters                    normal  size\n",
            "9          she      has                      magic  fan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create graph\n",
        "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\",\n",
        "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos, node_size=200, font_size=6)\n",
        "edge_labels = {(source, target): attr['edge'] for source, target, attr in G.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(G, pos=pos, edge_labels=edge_labels, font_size=5)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pPjNdLHoIh-t",
        "outputId": "13a80d81-c1b9-47db-93dd-7bbb9725ff24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1221d56e4824>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\",\n\u001b[0m\u001b[1;32m      3\u001b[0m                           edge_attr=True, create_using=nx.MultiDiGraph())\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"When Alice finds a bottle labeled 'Drink me', she does.\"\n",
        "#!pip install spacy-transformers\n",
        "#nlp = spacy.load('en_core_web_trf')\n",
        "# Iterate through the tokens\n",
        "for token in doc:\n",
        "    # If the token is the main verb (\"finds\")\n",
        "    if token.dep_ != \"ROOT\":\n",
        "# Iterate through the root token's children\n",
        "      for child in token.children:\n",
        "        # Check if the dependent is an adjective clause modifier\n",
        "        if child.dep_ == \"acl\":\n",
        "            # Print the dependent and its subtree\n",
        "            #print(token, token.dep_, child.text, [(sub_token.text, sub_token.dep_, sub_token.pos_) for sub_token in child.subtree if sub_token.is_punct == False])\n",
        "            subtree_text = \"\"\n",
        "            for sub_token in child.subtree:\n",
        "                if not sub_token.is_punct:\n",
        "                    subtree_text += sub_token.text + \" \"\n",
        "            # Print the dependent and its subtree\n",
        "            #print(subtree_text.strip())\n"
      ],
      "metadata": {
        "id": "obDi4HEEQXal",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6c212212-5818-451b-ac0d-a6cd1792fdfd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'doc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-350c8b683823>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#nlp = spacy.load('en_core_web_trf')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# If the token is the main verb (\"finds\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ROOT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triples = kg_df.apply(lambda row: '; '.join(row), axis=1).tolist()\n",
        "context = \"(\"+\"),(\".join(triples)\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILyO4ZhOGzg0",
        "outputId": "1a7999bd-08ca-4029-e355-91c981a3dcd9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Alice; follows; white Rabbit hole),(Alice; finds; tiny  door),(When  Alice; does; bottle labeled Drink me),(Alice; shrinks; enough  door),(Alice; eats; then  something labeled Eat me),(Alice; finds; fan),(that; enables; enough  Garden),(Dog; tries; her),(She; enters; normal  size),(she; has; magic  fan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VssWLaewg_W_",
        "outputId": "bf964a86-b0c8-4f9a-a60f-378c5cb7e642"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from constants import MAX_NEW_TOKENS, MODEL_FAMILY_DICT\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_FAMILY_DICT = {\n",
        "    # embeddings\n",
        "    \"text-embedding-ada-002\": \"openai\",\n",
        "    \"all-MiniLM-L6-v2\": \"sentence-transformers\",\n",
        "    \"BAAI/bge-base-en-v1.5\": \"sentence-transformers\",\n",
        "    \"BAAI/bge-large-en-v1.5\": \"sentence-transformers\",\n",
        "    # generators\n",
        "    \"gpt-35-turbo\": \"openai\",\n",
        "    \"gpt-35-turbo-16k\": \"openai\",\n",
        "    \"gpt-4\": \"openai\",\n",
        "    \"gpt-4-0613\": \"openai\",\n",
        "    \"mistralai/Mistral-7B-v0.1\": \"transformers\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\": \"transformers\",\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.2\": \"transformers\",\n",
        "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": \"transformers\",\n",
        "    \"microsoft/phi-2\": \"transformers\",\n",
        "    \"gemini-pro\": \"google\",\n",
        "}\n",
        "\n",
        "def HuggingFaceGenerator(model_name, MAX_NEW_TOKENS):\n",
        "  max_new_tokens = MAX_NEW_TOKENS\n",
        "\n",
        "        # model = self.generator_name\n",
        "        # # model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "           load_in_4bit=True,\n",
        "            torch_dtype=torch.float16,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            )\n",
        "\n",
        "def HuggingGenerate(prompt, max_length=100, num_return_sequences=1):\n",
        "  inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "  outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "  answer = self.tokenizer.batch_decode(outputs)[0][len(prompt):]\n",
        "  return answer"
      ],
      "metadata": {
        "id": "hxoOoXLreAOE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bitsandbytes\n",
        "triples = kg_df.apply(lambda row: '; '.join(row), axis=1).tolist()\n",
        "context = \"(\"+\"),(\".join(triples)\n",
        "\n",
        "# Generator\n",
        "generator = HuggingFaceGenerator(\"mistralai/Mistral-7B-Instruct-v0.1\", 32)\n",
        "while True:\n",
        "    query = input(\"Ask a question: \")\n",
        "    if query == \"exit\":\n",
        "      break\n",
        "\n",
        "    from pprint import pprint\n",
        "    pprint(context)\n",
        "\n",
        "    RAG_prompt = f\"\"\"You are an assistant for question-answering tasks.\n",
        "            The context is given to you in the form of (source; relation; target).\n",
        "            If you don't know the answer, just say that you don't know.\n",
        "            Use only the context to answer.\n",
        "            Question: {query}\n",
        "            Context: {context}\n",
        "            Answer:\n",
        "        \"\"\"\n",
        "    #answer = generator.generate(RAG_prompt)\n",
        "    #print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4XWYQZ8Fe8Q7",
        "outputId": "aa31ee30-0a3c-42cd-98fc-12c966434824"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'kg_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b94cc0a369ea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install bitsandbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtriples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'; '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"(\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"),(\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kg_df' is not defined"
          ]
        }
      ]
    }
  ]
}